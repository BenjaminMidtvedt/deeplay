{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MNIST Classification\n",
        "\n",
        "We will build a dense neural network to classify images and apply it to the classical problem of the classification of the hand-written digits in the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The MNIST dataset\n",
        "\n",
        "The MNIST dataset consists of grayscale images of handwritten digits from 0 to 9. Each image is 28 pixels by 28 pixels. There are 60,000 training images and 10,000 test images.\n",
        "\n",
        "We have organized these images in two folders named `train` and `test`:\n",
        "\n",
        "> train/0_000000.png<br>\n",
        "> train/0_000001.png<br>\n",
        "> ...<br>\n",
        "> train/1_000000.png<br>\n",
        "> ...<br>\n",
        "\n",
        "> test/0_000000.png<br>\n",
        "> ...<br>\n",
        "> test/1_000000.png<br>\n",
        "> ...<br>\n",
        "\n",
        "The first digit in the filename is the label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# downloads the repository containing the MINST dataset \n",
        "# only if the MNIST_dataset directory is empty\n",
        "os.system(\"git clone https://github.com/DeepTrackAI/MNIST_dataset\")\n",
        "\n",
        "train_path = os.path.join(\"MNIST_dataset\", \"mnist\", \"train\")\n",
        "test_path = os.path.join(\"MNIST_dataset\", \"mnist\", \"test\")\n",
        "\n",
        "train_images_paths = os.listdir(train_path)\n",
        "test_images_paths = os.listdir(test_path)\n",
        "\n",
        "print(f\"{len(train_images_paths)} training images\")\n",
        "print(f\"{len(test_images_paths)} test images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `matplotlib` to load the images into `numpy` arrays. Since the `mnist` dataset is small enough to be contained in memory, we can load all images at once. For larger datasets, we would need to load the images as needed during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_images = []\n",
        "for path in train_images_paths:\n",
        "    image = plt.imread(os.path.join(train_path, path))\n",
        "    train_images.append(image)\n",
        "\n",
        "test_images = []\n",
        "for path in test_images_paths:\n",
        "    image = plt.imread(os.path.join(test_path, path))\n",
        "    test_images.append(image)\n",
        "\n",
        "print(f\"{len(train_images)} training images with shape {train_images[0].shape}\")\n",
        "print(f\"{len(test_images)} test images with shape {test_images[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the digit is encoded as the first character of the filename, we can extract the ground-truth labels from the filenames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ground_truth = []\n",
        "for path in train_images_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    digit = int(filename[0])\n",
        "    train_ground_truth.append(digit)\n",
        "\n",
        "test_ground_truth = []\n",
        "for path in test_images_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    digit = int(filename[0])\n",
        "    test_ground_truth.append(digit)\n",
        "\n",
        "print(f\"{len(train_ground_truth)} training ground-truth digits\")\n",
        "print(f\"{len(test_ground_truth)} test ground-truth digits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now visualize some of the MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indexes_of_images_to_show = np.linspace(0, 60000, 18, dtype=int, endpoint=False)\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "\n",
        "for i in range(18):\n",
        "    data_index = indexes_of_images_to_show[i]\n",
        "    train_image = train_images[data_index]\n",
        "    train_groundtruth = train_ground_truth[data_index]\n",
        "\n",
        "    plt.subplot(3, 6, i + 1)\n",
        "    plt.title(f\"Label: {train_groundtruth}\", fontsize=20)\n",
        "    plt.imshow(train_image.squeeze(), cmap=\"Greys\") \n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Neural Network\n",
        "\n",
        "We will create the neural network using `deeplay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Model\n",
        "\n",
        "We will start with a small _dense neural network_ (often called also _multi-layer perceptron_ or _fully connected neural network_) with 784 = 28 x 28 inputs (one for each pixel, `.in_features(28 * 28)`), two hidden layers with 32 neurons each (`.hidden_dims([32, 32])`) with _sigmoid_activation (`.blocks.activation(Sigmoid)`), and an output layer with 10 neurons (`.out_features(10)`) with _sigmoid_ activation (one for each digit, `.out_activation(Sigmoid)`). \n",
        "\n",
        "The output will be a vector of 10 numbers between 0 and 1, which can be loosely interpreted as probabilities, so that the predicted digit is the one with the highest output value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deeplay as dl\n",
        "from torch.nn import Sigmoid\n",
        "\n",
        "mlp_config = (dl.Config()\n",
        "    .in_features(28 * 28)\n",
        "    .hidden_dims([32, 32])\n",
        "    .blocks.activation(Sigmoid)\n",
        "    .out_features(10)\n",
        "    .out_activation(Sigmoid)\n",
        "    .blocks.activation(Sigmoid)\n",
        ")\n",
        "mlp_model = dl.MultiLayerPerceptron.from_config(mlp_config)\n",
        "\n",
        "print(mlp_model)\n",
        "print(f\"{sum(p.numel() for p in mlp_model.parameters())} trainable parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Application as Classifier\n",
        "\n",
        "We use the `Classifier` application class from `deeplay` to define what we want to do with the neural network (e.g., determining the training logic and adding some convenient metrics such as accuracy).\n",
        "\n",
        "We add to the classifier the model we have jsut created (`.model(dt.MultiLayerPerceptron, mlp_config)`). Then, we set the numebr of classes (`.num_classes(10)`), convert the ground-truth labels to one-hot vectors to match the output of the model (`.make_target_onehot(True)`), set the mean squared error as loss function (`.loss(MSELoss)`), and set stochastic gradient descent as optimizer with learning rate `lr=0.1` (`.optimizer(SGD, lr=0.1)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "\n",
        "classifier_config = (dl.Config()\n",
        "    .model(dl.MultiLayerPerceptron, mlp_config)\n",
        "    .num_classes(10)\n",
        "    .make_target_onehot(True)\n",
        "    .loss(MSELoss)\n",
        "    .optimizer(SGD, lr=0.1)\n",
        ")\n",
        "classifier = dl.Classifier.from_config(classifier_config)\n",
        "    \n",
        "print(classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**\n",
        "\n",
        "The config above is equivalent to the following, where the properties of the neural network model are set explicitly.\n",
        "\n",
        "```python\n",
        "classifier_config = (\n",
        "    dl.Config()\n",
        "    .model(dl.MultiLayerPerceptron)\n",
        "    .model.in_features(28 * 28)\n",
        "    .model.hidden_dims([32, 32])\n",
        "    .model.out_features(10)\n",
        "    .model.out_activation(Sigmoid)\n",
        "    .model.blocks.activation(Sigmoid)\n",
        "    .num_classes(10)\n",
        "    .make_targets_onehot(True)\n",
        "    .loss(MSELoss)\n",
        "    .optimizer(SGD, lr=0.1)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloader\n",
        "\n",
        "A `DataLoader` object manages the data to be passed to the neural network. The simplest way to create a dataloader from data already in memory is to create a list of `(sample, ground_truth)` tuples. This can easily be achieved using the Python native function `zip()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_mnist_image_and_groundtruth = list(zip(train_images, train_ground_truth))\n",
        "test_mnist_image_and_groundtruth = list(zip(test_images, test_ground_truth))\n",
        "\n",
        "mnist_train_dataloader = dl.DataLoader(train_mnist_image_and_groundtruth, shuffle=True) # important to shuffle the data - try out what happens if you do not\n",
        "mnist_test_dataloader = dl.DataLoader(test_mnist_image_and_groundtruth, shuffle=False)\n",
        "\n",
        "print(f\"{len(mnist_train_dataloader)} training batches\")\n",
        "print(f\"{len(mnist_test_dataloader)} test batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trainer\n",
        "\n",
        "A `Trainer` object manages the training and evaluation, e.g., deciding what hardware to use, when to stop training, whether to utilize the GPU, and when to save the model, log the training, and evaluate metrics. \n",
        "\n",
        "For now, we will create a trainer for a single epoch (an epoch is a single pass through the entire training set, `max_epochs=1`) and with automatic hardware acceleration (it will use a GPU if available, otherwise a CPU, `accelerator=\"auto`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(\n",
        "    max_epochs=1,\n",
        "    accelerator=\"auto\", \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(classifier, mnist_train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the test set. We can reuse the trainer to test the model on the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.test(classifier, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "A _confusion matrix_ can be used to visualize the performance of a classifier. It indicates what errors the neural network makes and to gain insights that can help us improve its architecture and training hyperparameters. \n",
        "\n",
        "A confusion matrix is a square matrix with a number of rows and columns equal to the number of classes in the classification problem, here 10. \n",
        "Each row and column corresponds to one class in the classification problem, ordered arbitrarily but equally for the two axes. \n",
        "In this case, we have a natural ordering of the classes, which is the order of the digits. \n",
        "An element c_i,j of the confusion matrix represents the number of times the neural network assigned the predicted class j to the actual class i. For example, in our case, c_3,5 would correspond to the number of times the neural network, when given an image depicting the digit 3, classified it as the digit 5. The name stems from the fact that this representation makes it easy to see which classes the model is most commonly confusing.\n",
        "\n",
        " Here, we have chosen to assign the `groundtruth_digit` to the second axis and `predicted_digit` to the first axis, but the opposite is also a valid (and comon) choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from seaborn import heatmap, cubehelix_palette\n",
        "from numpy import zeros\n",
        "\n",
        "trainer.predict\n",
        "\n",
        "# Compute the confusion matrix\n",
        "def plot_confusion_matrix(model, trainer, dataloader):\n",
        "    confusion_matrix = zeros((10, 10), dtype=int)\n",
        "\n",
        "    for image, groundtruth_digits in dataloader:\n",
        "        predicted_probabilities = model(image)\n",
        "        max_probability, predicted_digits = predicted_probabilities.max(dim=1)\n",
        "        \n",
        "        # np.add.at is a function that allows us to add values to a numpy array at specific indices\n",
        "        # Unlike arr[indices] += 1, np.add.at(arr, indices, 1) handles the case where there are duplicate indices.\n",
        "        # So, arr[[0, 0]] += 1 would result in arr[0] = 1, but np.add.at(arr, [0, 0], 1) results in arr[0] = 2 \n",
        "        np.add.at(confusion_matrix, (groundtruth_digits, predicted_digits), 1)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    heatmap(\n",
        "        confusion_matrix,\n",
        "        annot=True, \n",
        "        fmt=\".0f\", \n",
        "        square=True, \n",
        "        cmap=cubehelix_palette(light=0.95, as_cmap=True),\n",
        "        vmax=150,\n",
        "    )\n",
        "    plt.xlabel(\"Predicted digit\", fontsize=15)\n",
        "    plt.ylabel(\"Ground truth digit\", fontsize=15)\n",
        "    plt.show()\n",
        "    \n",
        "plot_confusion_matrix(classifier, trainer, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Improvements\n",
        "\n",
        "We now start a journey to improve our network and its training, as is usual in developing machine-learning solutions.\n",
        "We will gradually introduce some new (and fundametal) concepts, to demonstrate how they affect the performance of the trained network, which provide valuable intuition for what hyperparameters are relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Representation\n",
        "\n",
        "Currently each digit is assigned a probability between 0 and 1 by the network. However, we know that only one classification is true. As such, we can normalize the output such that the prediction sums to one. By doing so, we impose our prior knowledge to reduce the complexity of the problem. This is typically done using what is known as _softmax activation_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import RMSprop\n",
        "import torch\n",
        "\n",
        "classifier_config_with_softmax_out = (\n",
        "    classifier_config\n",
        "    .model.out_activation(torch.nn.Softmax, dim=1)\n",
        ")\n",
        "\n",
        "classifier = dl.Classifier.from_config(classifier_config_with_softmax_out)\n",
        "print(classifier)\n",
        "\n",
        "trainer = dl.Trainer(\n",
        "    max_epochs=1,\n",
        "    accelerator=\"auto\", \n",
        ")\n",
        "\n",
        "trainer.fit(classifier, mnist_train_dataloader)\n",
        "trainer.test(classifier, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(classifier, trainer, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_config_with_relu_internal = (\n",
        "    classifier_config_with_softmax_out\n",
        "    .model.blocks.activation(torch.nn.ReLU)\n",
        ")\n",
        "\n",
        "classifier = dl.Classifier.from_config(classifier_config_with_relu_internal)\n",
        "print(classifier)\n",
        "\n",
        "trainer = dl.Trainer(\n",
        "    max_epochs=1,\n",
        "    accelerator=\"auto\", \n",
        ")\n",
        "\n",
        "trainer.fit(classifier, mnist_train_dataloader)\n",
        "trainer.test(classifier, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(classifier, trainer, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini Batches\n",
        "\n",
        "Let us we will introduce our next improvement: _mini-batches_. Mini-batches are a way to train neural networks using only a subset of the training data at each iteration. This is done by splitting the training data into smaller subsets called _mini-batches_. The neural network is then trained on each mini-batch in turn, and the parameters are updated after each mini-batch. In fact, we were already using mini-batches, but we were using a batch size of one. The batch size is a hyperparameter that can be tuned to improve the performance of the neural network. In general, larger batches can lead to faster training since more data is processed at once, but it demands more memory and might not always lead to better convergence. Moreover, while larger batches yield a better estimate of the local gradient, this does not necessarily correlate to allowing a larger weight update if the optimization landscape is rough. On the other hand, smaller batches can offer a regular update to the model and often lead to a more robust convergence, but the training might be slower due to the overhead of frequent updates. Thus, neither bigger nor smaller batches are universally superior; it's a balance of factors like memory constraints, convergence behavior, and training speed. As a rule of thumb, a batch size of `32` is a good starting point. Rarely are batches smaller than `8` or larger than `256` used.\n",
        "\n",
        "We set our batch size in our dataloaders. They will handle splitting the data into mini-batches for us. We will use a batch size of `16` for both the training and validation dataloaders. Moreover, since we are now using mini-batches, we can increase the number of epochs to `10` to compensate for the smaller batch size without significantly increasing the total training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_test_dataloader = dl.DataLoader(test_mnist_image_and_groundtruth, batch_size=16, shuffle=False)\n",
        "mnist_train_dataloader = dl.DataLoader(train_mnist_image_and_groundtruth, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer\n",
        "\n",
        "Using plain _stochastic gradient descent_ for updating weights is straightforward, but generally not optimal. \n",
        "Modern deep learning almost universally uses extensions such as _root mean squared propagation_ (RMSprop) or _Adam_. The main advantages of these alternatives are that they change the learning rate for each weight individually during training. Each algorithm has its own way of doing so, but it is common to use some type of _momentum_. For example, a weight that is updated in the same direction multiple times in a row will take larger steps at each iteration, while a weight that changes direction frequently will take smaller steps. To better utilize our increased batch size, we will use the `RMSprop` optimizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "config_with_rmsprop = (\n",
        "    classifier_config_with_relu_internal\n",
        "    .optimizer(RMSprop, lr=0.001)\n",
        ")\n",
        "\n",
        "classifier = dl.Classifier.from_config(config_with_rmsprop)\n",
        "print(classifier)\n",
        "\n",
        "trainer = dl.Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=\"auto\", \n",
        ")\n",
        "\n",
        "trainer.fit(classifier, mnist_train_dataloader)\n",
        "trainer.test(classifier, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(classifier, trainer, mnist_test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Failure Analysis\n",
        "\n",
        "it is often instructive to check for which inputs the network fails. In fact, this can give further insights into what is still difficult for the neural network to classify and, therefore, suggests potential ways to improve its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_test_dataloader = dl.DataLoader(train_mnist_image_and_groundtruth, batch_size=1, shuffle=False)\n",
        "\n",
        "number_of_images_per_digit_to_show = 10\n",
        "num_incorrect_predictions_per_digit = np.zeros(10, int)\n",
        "\n",
        "plt.figure(figsize=(10, number_of_images_per_digit_to_show))\n",
        "\n",
        "for image, groundtruth_digits in mnist_test_dataloader:\n",
        "    groundtruth_digits = int(groundtruth_digits)\n",
        "\n",
        "    if num_incorrect_predictions_per_digit[groundtruth_digits] >= number_of_images_per_digit_to_show:\n",
        "        continue\n",
        "\n",
        "    predicted_probabilities = classifier(image)\n",
        "    max_probability, predicted_digits = predicted_probabilities.max(dim=1)\n",
        "    if predicted_digits == groundtruth_digits:\n",
        "        continue \n",
        "\n",
        "    num_incorrect_predictions_per_digit[groundtruth_digits] += 1\n",
        "\n",
        "    subplot_idx = groundtruth_digits * number_of_images_per_digit_to_show + num_incorrect_predictions_per_digit[groundtruth_digits]\n",
        "    plt.subplot(10, number_of_images_per_digit_to_show, subplot_idx)\n",
        "    plt.imshow(image.squeeze(), cmap=\"Greys\")\n",
        "    plt.annotate(str(int(predicted_digits)), (0.8, 1), (1, 1), xycoords=\"axes fraction\", textcoords=\"offset points\", va=\"top\", ha=\"left\", fontsize=20, color=\"red\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    if (num_incorrect_predictions_per_digit >= number_of_images_per_digit_to_show).all():\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
